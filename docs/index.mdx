---
title: "Jira MCP"
description: "Control Jira through AI-powered LLM clients using the Model Context Protocol"
---

![Claude Demo](/images/claude.gif)

<Note>
  **Safe for corporate environments.** This MCP server runs completely locally
  on your machine using the [jira-cli](https://github.com/ankitpokhrel/jira-cli)
  command line tool. No data is sent to third-party services. If your
  organization approves the use of jira-cli, on a technical level this MCP
  server is safe to use. _If in doubt, check with your IT department._
</Note>

Allows AI assistants like Cursor, Claude Desktop, Windsurf, or ChatGPT to ask questions and perform actions with your Jira instance.

## Example prompts

<CardGroup cols={2}>
  <Card
    title="Show tickets assigned to me with links and a summary of progress."
    icon="list-check"
  />
  <Card
    title="What's the link and status of that ticket with the new button feature?"
    icon="circle-question"
  />
  <Card
    title="Add a comment to PROJ-456 explaining the fix in my current branch."
    icon="comment"
  />
  <Card
    title="Add this spreadsheet of user stories as story issues to PROJ. Make the stories medium priority with a due date of 2 weeks from now."
    icon="file-import"
  />
</CardGroup>

## Get started

<Card title="Quickstart" icon="rocket" href="/quickstart" horizontal>
  Install and configure Jira MCP in minutes.
</Card>

## Configure your AI tool

Set up Jira MCP with your preferred AI assistant.

<CardGroup cols={3}>
  <Card title="Cursor" icon="arrow-pointer" href="/ai-tools/cursor">
    Configure Cursor IDE with Jira MCP.
  </Card>
  <Card title="Claude Code" icon="asterisk" href="/ai-tools/claude-code">
    Set up Claude Code CLI with Jira MCP.
  </Card>
  <Card title="Windsurf" icon="water" href="/ai-tools/windsurf">
    Configure Windsurf with Jira MCP.
  </Card>
</CardGroup>

## How it works

<Steps>
<Step title="Prompt your AI assistant">
  You enter questions or commands to an LLM client such as Claude Desktop, Cursor, Windsurf, or ChatGPT.
</Step>

<Step title="LLM analyzes available tools">
  The LLM analyzes the available MCP tools and decides which one(s) to use. The
  LLM has context of each tool and what it is meant for in human language.
</Step>

<Step title="MCP server executes commands">
  The client executes the chosen tool(s) through the MCP server. The MCP server
  runs locally on your machine or remotely via an endpoint.
</Step>

<Step title="Results returned to LLM">
  The results are sent back to the LLM, which formulates a natural language response and displays data or performs actions using the MCP server.
</Step>
</Steps>

## Architecture

MCP follows a client-server architecture where an **MCP host** (an AI application like Cursor or Claude Desktop) establishes connections to one or more **MCP servers**. The **MCP host** accomplishes this by creating one **MCP client** for each **MCP server**. Each MCP client maintains a dedicated connection with its corresponding MCP server.

<Card
  title="Learn more about MCP architecture"
  icon="diagram-project"
  href="https://modelcontextprotocol.io/docs/learn/architecture"
>
  Read the official MCP documentation for details on architecture.
</Card>
